# -*- coding: utf-8 -*-
"""Uber Travels

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qoozkxufbEVOCtfx8imJLL1wwbGXhuBl
"""

import geopandas as gpd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import folium
from folium.plugins import HeatMap, MarkerCluster
from geopy.distance import geodesic

data = pd.read_csv('/content/Travels.csv')

data.info()

# Checking data quality
# Check for missing values in the dataset
missing_values = data.isnull().sum()

# Calculate the percentage of missing values for each column
missing_percentage = (missing_values / len(data)) * 100

# Combine the number of missing values and their percentage into a DataFrame
missing_data_info = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})

missing_data_info[missing_data_info['Missing Values'] > 0]  # Display only columns with missing values

# Remove columns with mostly missing values
data = data.drop(columns=['free_credit_used', 'charity_id','rating','end_zip_code', 'start_zip_code','HeavyFog','surge_factor' ])

rows_with_missing_values = data[data.isnull().any(axis=1)]

print(rows_with_missing_values)

# Remove rows with missing values (2 rows)
data = data.dropna(subset=['Fog'])
data = data.dropna(subset=['distance_travelled'])

# Checking for duplicate rows
duplicate_rows = data.duplicated().sum()
duplicate_rows

# Remove columns with duplicate values
data = data.drop_duplicates()

# Handling missing values for 'driver_rating' and 'rider_rating' by imputing with the median
data['driver_rating'].fillna(data['driver_rating'].median(), inplace=True)
data['rider_rating'].fillna(data['rider_rating'].median(), inplace=True)

data.info()

data.head()

# Summary statistics for numerical features
numerical_summary = data.describe()
numerical_summary

# Convert the 'distance_travelled' column from meters to kilometers
data['distance_travelled'] = data['distance_travelled'] / 1000

data['year'].unique()

data['year'] = data['year'].replace(2104, 2014)

# Calculate the duration of each travel in hours
data['completed_on'] = pd.to_datetime(data['completed_on'])
data['started_on'] = pd.to_datetime(data['started_on'])

data['travel_duration'] = round((data['completed_on'] - data['started_on']).dt.total_seconds() / 3600, 2)

# Keeping trip duration in minutes for easier calculations
data['duration_minutes'] = data['travel_duration'] * 60

data.describe()

mask = data['started_on'] <= data['completed_on']

# Apply the mask to keep only the rows where started_on is not newer than completed_on
data = data[mask]

"""There are many zero minute durations."""

# Filter rows where distance_travelled is not zero and trip_duration_minutes is zero
zero_duration_rows = data[(data['distance_travelled'] != 0) & (data['duration_minutes'] == 0)]

# Check if there are any zero_duration_rows records
if not zero_duration_rows.empty:
    print("Inconsistent records found:")
    print(zero_duration_rows)
else:
    print("No inconsistent records found.")

"""I saw here that many timestamps for startDate and endDate in my data are the same, meaning there is going to be many zero durations even though they have nonzero distances. So I will adjust them based on the distance traveled and the duration of similar travels.

"""

#Find similar travel records
tolerance = 0.005  # Defining a tolerance limit for distance
similar_travel_records = data[(abs(data['distance_travelled'] - zero_duration_rows.iloc[0]['distance_travelled']) <= tolerance) & (data['duration_minutes'] > 0)]

# Calculate a new 'completed_on' for identical start and end timestamp rows
if not similar_travel_records.empty:
    mean_duration = similar_travel_records['duration_minutes'].mean()
    # Update directly on the original DataFrame
    for index, row in zero_duration_rows.iterrows():
        new_end_time = row['started_on'] + pd.to_timedelta(mean_duration, unit='min')
        data.loc[index, 'completed_on'] = new_end_time
else:
    print("No similar records found to calculate mean duration.")

# Check if there are any zero distance records
zero_distance = data[(data['distance_travelled'] == 0)]
zero_distance

data = data[data['distance_travelled'] != 0]

# Calculate basic statistics
total_trips = data.shape[0]
date_range = (data['started_on'].min(), data['completed_on'].max())
summary_distance = data['distance_travelled'].describe()
summary_duration = data['travel_duration'].describe()
summary_driver_rating = data['driver_rating'].describe()
summary_rider_rating = data['rider_rating'].describe()

# Compile the basic statistics into a dictionary
basic_stats = {
    "Total Trips": total_trips,
    "Date Range": date_range,
    "Distance Travelled": summary_distance,
    "Trip Duration": summary_duration,
    "Driver Rating": summary_driver_rating,
    "Rider Rating": summary_rider_rating
}

basic_stats

# Selecting relevant columns for correlation analysis
correlation_data = data[['driver_rating', 'rider_rating', 'Date','distance_travelled', 'travel_duration', 'AWND', 'GustSpeed2', 'Fog', 'Thunder']]

# Calculate the correlation matrix
correlation_matrix = correlation_data.corr()

# Plot the correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# Weather Impact Analysis

# Selecting weather-related columns for analysis
weather_columns = ['PRCP', 'TMAX', 'TMIN', 'AWND', 'GustSpeed2', 'Fog', 'Thunder']

# Correlation of weather conditions with distance traveled
weather_distance_correlation = data[weather_columns + ['distance_travelled']].corr()['distance_travelled']

# Correlation of weather conditions with trip duration
weather_duration_correlation = data[weather_columns + ['duration_minutes']].corr()['duration_minutes']

# Plotting
plt.figure(figsize=(12, 5))

# Correlation with Distance Travelled
plt.subplot(1, 2, 1)
weather_distance_correlation.drop('distance_travelled').plot(kind='bar')
plt.title('Correlation with Distance Travelled')
plt.xlabel('Weather Conditions')
plt.ylabel('Correlation Coefficient')

# Correlation with Duration Minutes
plt.subplot(1, 2, 2)
weather_duration_correlation.drop('duration_minutes').plot(kind='bar')
plt.title('Correlation with Trip Duration')
plt.xlabel('Weather Conditions')
plt.ylabel('Correlation Coefficient')

plt.tight_layout()
plt.show()

data['avg_temp'] = (data['TMIN'] + data['TMAX']) / 2
data['avg_temp']

# Weather Impact: Scatter plot of TMAX vs. Average Number of Trips
# Group the data by 'Completion Date' and get the average number of trips for each temperature value
weather_impact_dat = data.groupby('avg_temp')['completed_on'].count().reset_index()
weather_impact_dat.columns = ['avg_temp', 'Average_Trips']

plt.figure(figsize=(10, 6))
sns.kdeplot(data=weather_impact_dat, x='avg_temp', y='Average_Trips', fill=True, cmap="Reds", alpha=0.5)
plt.title('Density Plot of Temperature vs. Average Number of Trips')
plt.xlabel('Temperature (Â°F)')
plt.ylabel('Average Number of Trips')
plt.show()

# Create a heatmap of trips by hour and day of the week

# Extracting the day of the week and hour from the 'started_on' column
data['weekday'] = data['started_on'].dt.day_name()
data['hour'] = data['started_on'].dt.hour

# Create a pivot table to count trips by hour and weekday
pivot_table_time = data.pivot_table(index='hour', columns='weekday', values='started_on', aggfunc='count')

# Order the days of the week
ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
pivot_table_time = pivot_table_time[ordered_days]

# Plotting the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table_time, cmap='viridis', linewidths=.5)
plt.title('Heatmap of Trips by Hour and Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Hour of Day')
plt.show()

# Creating a new column for the month and year for further analysis
data['month_year'] = data['completed_on'].dt.to_period('M')

# Aggregating trip data by month
monthly_trips = data['month_year'].value_counts().sort_index()

# Plotting the monthly trip distribution
plt.figure(figsize=(10, 6))
monthly_trips.plot(kind='bar', color='purple')
plt.title('Monthly Trips Distribution')
plt.xlabel('Month')
plt.ylabel('Number of Trips')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Rating Analysis

# Distribution of Driver Ratings
driver_rating_distribution = data['driver_rating'].value_counts().sort_index()

# Distribution of Rider Ratings
rider_rating_distribution = data['rider_rating'].value_counts().sort_index()

# Plotting
plt.figure(figsize=(12, 5))

# Driver Rating Distribution
plt.subplot(1, 2, 1)
driver_rating_distribution.plot(kind='bar')
plt.title('Driver Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.xticks(rotation=0)

# Rider Rating Distribution
plt.subplot(1, 2, 2)
rider_rating_distribution.plot(kind='bar')
plt.title('Rider Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.xticks(rotation=0)

plt.tight_layout()
plt.show()

# Distribution of Requested Car Categories
category_distribution = data['requested_car_category'].value_counts()

plt.figure(figsize=(12, 5))

# Car Category Distribution
plt.subplot(1, 2, 1)
category_distribution.plot(kind='bar')
plt.title('Requested Car Category Distribution')
plt.xlabel('Car Category')
plt.ylabel('Number of Requests')
plt.xticks(rotation=45)

plt.show()

def geospatial_analyses(filepath):

    # Load the dataset into a Geo Dataframe
    gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data['end_location_long'], data['end_location_lat']))

    # Remove rows with missing or erroneous geospatial data
    gdf = gdf[gdf['geometry'].is_valid]

    # Create a base map with the location means
    m = folium.Map(location=[gdf['end_location_lat'].mean(), gdf['end_location_long'].mean()], zoom_start=2)

    # Add heatmap to the map
    heat_data = [[point.xy[1][0], point.xy[0][0]] for point in gdf.geometry]
    HeatMap(heat_data).add_to(m)

    # Add clustering to the map
    marker_cluster = MarkerCluster().add_to(m)
    for idx, row in gdf.iterrows():

        # Create a popup content for car info
        popup_content = f"Car Brand: {row['make']}<br>Car Model: {row['model']}<br>Car Year: {row['year']}<br>Trip Date: {row['completed_on']}"
        popup = folium.Popup(popup_content, max_width=450)

        folium.Marker(
            [row['end_location_lat'], row['end_location_long']],
            popup=popup
        ).add_to(marker_cluster)



     # Save the map
    m.save('geospatial_analyses.html')

    return m

geospatial_analyses('<PATH_TO_Travel.csv>')

from google.colab import files

# Download the file
files.download('trip_end_locations_map.html')